# ğŸ§  Archivio-Semantico-AI  
### Classificazione Archivistica Basata su Metadati (LLM Locale)

Sistema AI per **analisi semantica, classificazione e normalizzazione** di archivi documentali.  
Il progetto utilizza **LLM locali (on-premise)** per interpretare e mappare i metadati (percorsi, nomi file, campi CSV) secondo lo **schema del Titolario**, garantendo **massima privacy** e nessun invio di dati verso servizi cloud esterni.

---

## âœ¨ Caratteristiche Principali

| Aspetto | Descrizione |
|---|---|
| **Classificazione Contestuale** | Lâ€™LLM analizza percorso e nome file (es. `/nas/urbanistica/progetti/pratica_piazza_marconi/`) per assegnare il codice archivistico corretto. |
| **LLM 100% Locale** | Tutta lâ€™elaborazione avviene su server interni (vLLM o Ollama) con modelli come Llama o Mistral. Nessun cloud. |
| **Few-Shot Learning** | Addestramento con esempi reali del tuo archivio per massima accuratezza. |
| **Normalizzazione Dati** | Uniforma campi eterogenei dei CSV (es. â€œOggettoâ€, â€œMittenteâ€) in formati coerenti. |
| **Migrazione S3 Integrata** | Organizza automaticamente i file in MinIO (`s3://bucket/codice_titolario/progetto/`) con policy WORM. |

---

## ğŸ—ï¸ Architettura del Workflow

Il sistema gestisce due flussi basati su **analisi testuale dei metadati**:

### ğŸ”¹ Flusso 1: Classificazione File (Metadati Archiviazione)
1. **Ingestione:** scansione del filesystem (NAS) â†’ CSV con percorso e nome file.  
2. **Analisi LLM:** `analyze_files.py` invia i dati allâ€™LLM locale e riceve un JSON con classificazione archivistica.  
3. **Migrazione:** caricamento su MinIO organizzato per codice Titolario.

### ğŸ”¹ Flusso 2: Analisi Record Gestionali (CSV)
1. **Input CSV:** campi come â€œOggettoâ€, â€œMittenteâ€, â€œDataâ€.  
2. **Analisi LLM:** `analyze_records_llm.py` restituisce la classificazione normalizzata in JSON.  
3. **Output CSV:** archivio arricchito con i nuovi campi AI.

---

## âš™ï¸ Requisiti Tecnici

### ğŸ’» Hardware Raccomandato
| Componente | Specifiche Consigliate | Note |
|---|---|---|
| **GPU (per LLM)** | NVIDIA RTX 4070 / 5090 (12GB+ VRAM) | Per modelli di medie dimensioni. |
| **CPU** | Intel i9 / AMD Ryzen 9 | Elaborazioni parallele. |
| **Storage** | 1 TB NVMe Gen4 | Cache e indicizzazione. |

### ğŸ§© Software Essenziale
| Requisito | Versione | Descrizione |
|---|---|---|
| **Python** | â‰¥ 3.11 | Ambiente principale. |
| **LLM Host** | vLLM o Ollama | Esegue il modello locale come API. |
| **MinIO** | LTS | Storage S3 compatibile per archiviazione e retention. |

---

## ğŸ§  Configurazione Few-Shot e Titolario

### 1. **Titolario Archivistico**
Aggiorna:

config/prompts/titolario.txt

con la struttura gerarchica del tuo Titolario.

### 2. **Casi Studio (Few-Shot Examples)**
Fornisci esempi di addestramento per i tuoi pattern archivistici:

| Tipo | File | Descrizione |
|---|---|---|
| **File System** | `config/prompts/few_shot_files.json` | Coppie `Percorso/Nome File â†’ JSON Classificazione` |
| **CSV Gestionale** | `config/prompts/few_shot_records.json` | Coppie `Campi CSV â†’ JSON Normalizzato` |

---

## â–¶ï¸ Workflow Esecutivo

### A. Classificazione File
```bash
# 1. Indicizzazione
python scripts/ingest_data.py --source /mnt/nas/archivio --dest /mnt/nvme/cache --output data/output/indice.csv

# 2. Analisi LLM Locale
python scripts/analyze_files.py --input data/output/indice.csv --workers 8

# 3. Migrazione su MinIO
python scripts/migrate_to_minio.py --input data/output/analisi.csv --apply-retention

B. Analisi Record Gestionali (CSV)

python scripts/analyze_records_llm.py \
  --input data/input/archivio_gestionale_raw.csv \
  --output data/output/archivio_gestionale_classificato.csv \
  --workers 8


â¸»

ğŸ³ Deploy Locale con Docker

Il sistema Ã¨ completamente contenuto in Docker e puÃ² essere eseguito in LAN senza accesso a Internet.

ğŸ“¦ docker-compose.yml

version: "3.9"

services:
  llm:
    image: ollama/ollama:latest
    container_name: llm_local
    restart: unless-stopped
    volumes:
      - ./models:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - OLLAMA_MODELS=/root/.ollama
    command: ["serve"]

  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    volumes:
      - ./minio_data:/data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"

  app:
    build: ./app
    container_name: archivio_semantico
    restart: unless-stopped
    depends_on:
      - llm
      - minio
    volumes:
      - ./data:/app/data
      - ./config:/app/config
      - ./scripts:/app/scripts
    environment:
      - LLM_API=http://llm:11434/api/generate
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_USER=admin
      - MINIO_PASSWORD=admin123
    command: tail -f /dev/null

ğŸ“ Struttura Raccomandata del Progetto

Archivio-Semantico-AI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ ingest_data.py
â”‚       â”œâ”€â”€ analyze_files.py
â”‚       â”œâ”€â”€ analyze_records_llm.py
â”‚       â””â”€â”€ migrate_to_minio.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ titolario.txt
â”‚       â”œâ”€â”€ few_shot_files.json
â”‚       â””â”€â”€ few_shot_records.json
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ output/
â”‚   â””â”€â”€ cache/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

ğŸ Esempio app/Dockerfile

FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY scripts/ ./scripts/
COPY config/ ./config/
COPY data/ ./data/

CMD ["bash"]

ğŸ§¾ Esempio requirements.txt

requests
pandas
minio
tqdm

ğŸš€ Avvio del Sistema

# 1. Avvia tutti i servizi
docker compose up -d

# 2. Accedi al container app
docker exec -it archivio_semantico bash

# 3. Lancia gli script di analisi
python scripts/analyze_files.py --input data/output/indice.csv --workers 8


â¸»

ğŸ¤ Contributi

Contributi per:
	â€¢	ottimizzare lâ€™inferenza LLM locale (vLLM, Ollama, quantizzazione)
	â€¢	aggiungere supporto a nuovi formati di metadati
	â€¢	migliorare la gestione dei dataset e lâ€™interfaccia n8n

sono benvenuti!
Apri una pull request o segnala unâ€™issue.

â¸»

ğŸ“œ Licenza

Distribuito con licenza MIT.
Made with â¤ï¸ in Tuscany ğŸ‡®ğŸ‡¹

---

Vuoi che aggiunga anche un **file `.env` di esempio** per gestire in modo sicuro le credenziali (LLM, MinIO, ecc.) nel Docker Compose? Sarebbe utile per deploy in ambienti pubblici o multiutente.